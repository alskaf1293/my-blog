[
  {
    "objectID": "posts/post6/index.html",
    "href": "posts/post6/index.html",
    "title": "Derivation of the Klein Gordon Equation",
    "section": "",
    "text": "In classical mechanics, the total energy of a system can be written as\n\\[\nE = T + V = \\frac{1}{2} mv^2 + V = \\frac{p^2}{2m} + V\n\\]\nwhere \\(T\\) and \\(V\\) are the kinetic and potential energy of a system, respectively. In quantum mechanics, we promote these classical variables to Hermitian operators, where we use the standard substitutions in the position basis are\n\\[\n\\hat{E} = i \\hbar \\frac{\\partial}{\\partial t}\n\\]\n\\[\n\\hat{p} = - i \\hbar \\nabla\n\\]\nand the energy relation becomes\n\\[\ni \\hbar \\frac{\\partial}{\\partial t} = -\\frac{\\hbar^2}{2m}\\nabla^2+V\n\\]\nand multiplication of the wavefunction \\(\\psi\\) yields\n\\[\ni \\hbar \\frac{\\partial \\psi}{\\partial t} = (-\\frac{\\hbar^2}{2m}\\nabla^2+V) \\psi\n\\]\nthe Schrodinger Equation. For the free particle, we can forget about \\(V\\) and the relation reduces to\n\\[\ni \\hbar \\frac{\\partial \\psi}{\\partial t} = -\\frac{\\hbar^2}{2m}\\nabla^2 \\psi\n\\]\nWe derived this from a non-relativistic nergy relation. To introduce relativity into our theory, we can instead start from the relativistic energy relation for a free particle\n\\[\nE^2 = p^2c^2 + m^2c^4\n\\]\nand plug in our operators as before\n\\[\n- \\hbar^2 \\frac{\\partial^2}{\\partial t^2} = -\\hbar^2 \\nabla^2 c^2 + m^2 c^4\n\\]\n\\[\n- \\hbar^2 \\frac{\\partial^2}{\\partial t^2}\\psi = (-\\hbar^2 \\nabla^2 c^2 + m^2 c^4) \\psi\n\\]\nRearranging yields\n\\[\n(\\hbar^2 \\frac{\\partial^2}{\\partial t^2}-\\hbar^2 \\nabla^2 c^2 + m^2 c^4) \\psi = 0\n\\]\n\\[\n(\\frac{1}{c^2} \\frac{\\partial^2}{\\partial t^2}- \\nabla^2 + \\frac{m^2 c^2}{\\hbar^2}) \\psi = 0\n\\]\nThis is known a the Klein-Gordon equation. Historically, other physicists at the time were pretty well satisfied with this equation as a relativistic version of the Schrodinger Equation. However, a clever man by the name of Paul Dirac thought that there were problems with this equation. Firstly, it was second order in time, which means that you’d need to know the initial state \\(\\psi(0)\\) and it’s first derivative \\(\\psi'(0)\\) in order to evolve it in time. Secondly, for reasons that I won’t get into here, being second order in time means that interpreting the probability of the Klein Gordon equation allows negative probability, which is nonsensical."
  },
  {
    "objectID": "posts/post5/index.html",
    "href": "posts/post5/index.html",
    "title": "The Distribution Shift Problem in LLMs",
    "section": "",
    "text": "In preparation for graduate school and future research, I have decided to learn more about statistical learning theory, which is an area of machine learning that I haven’t really looked at up until now. An interesting case study is the problem of LLM hallucination, which is a problem that any person who has spent a sufficient amount of time around chatbots has probably encountered.\nAn LLM is self-supervised learning mechanism, where a model learns to predict the next token given a sequence of previous tokens. The sentence “The current president is Donald” should be met with an output token of “Trump”. In reality, these tokens usually aren’t words, but character bits (ie. ‘ae’, ‘ee’, ‘e’, etc.) and the bits to use in the dictionary is decided by information theory.\nLet\n\\(X\\) be the space of possible token sequences\n\\(Y\\) be the space of possible next tokens\n\\(P_{\\text{train}}(x,y)\\) be the training distribution\n\\(P_{\\theta}(y | x)\\) be the conditional distribution over the next token sequences given a sequence\n\\(\\mathcal{L}(\\theta) = \\mathbb{E}_{(x,y) \\sim P_{train}(x,y)}[-\\log P_{\\theta}(y \\mid x)]\\)\nIn machine learning, the model minimizes \\(\\mathcal{L}\\) over time. The so called “out of distribution” problem happens upon deployment, when\n\\[\nP_{\\text{train}}(x,y) \\neq P_{\\text{test}}(x,y)\n\\]\nwhich yields hallucination and inability to derive new science. This is in contrast the assumption that underlies most theoretical and experimental machine learning that the testing and training distribution are the same. A measure for this distribution difference can be given by the Kullback-Leibler divergence\n\\[\nD_{\\mathrm{KL}}(P_{\\text{train}} \\,\\|\\, P_{\\text{test}})\n\\]"
  },
  {
    "objectID": "posts/post3/index.html",
    "href": "posts/post3/index.html",
    "title": "The Expectation Value of An Observable in Quantum mechanics",
    "section": "",
    "text": "In quantum mechanics, an observable is given by the Hermitian operator \\(\\hat{Q}\\). The expectation value of said observable is given by\n\\[\n\\langle Q \\rangle = \\bra{\\psi} \\hat{Q} \\ket{\\psi}\n\\]\n\\(\\ket{\\psi}\\) can be expanded in the basis of the observable, given with the coefficients\n\\[\n\\ket{\\psi} = c_1\\ket{q_1} + c_2\\ket{q_2} + \\dots + c_k \\ket{q_k}\n\\]\n\\[\n\\hat{Q}\\ket{\\psi} = \\hat{Q}(c_1\\ket{q_1} + c_2\\ket{q_2} + \\dots + c_k \\ket{q_k})\n\\] and given with the observable relation \\(\\hat{Q}\\ket{q_j} = q_j\\ket{q_j}\\), we have\n\\[\n\\bra{\\psi} \\hat{Q} \\ket{\\psi} = (c^*_1\\bra{q_1} + c^*_2\\bra{q_2} + \\dots + c^*_k \\bra{q_k}) \\left(c_1 q_1\\ket{q_1} + c_2 q_2\\ket{q_2} + \\dots + c_k q_k\\ket{q_k} \\right) \\\\\n\\]\n\\[\n\\langle Q \\rangle = \\sum_{i,j} c^*_i c_j q_j \\braket{q_i|q_j}\n\\]\nand using the orthonormality condition \\(\\braket{q_i|q_j} = \\delta_{ij}\\), we have\n\\[\n\\langle Q \\rangle = \\sum_{j} |c_j|^2 q_j\n\\]\nThis corresponds exactly to the expectation value equation."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Timothy Oh",
    "section": "",
    "text": "Crossroads\n\n\n\nblog\n\n\n\n\n\n\n\n\n\nJan 18, 2026\n\n\n\n\n\n\n\n\n\n\n\n\nNon Degenerate First Order Perturbation Theory\n\n\n\nquantum mechanics\n\n\n\n\n\n\n\n\n\nJan 14, 2026\n\n\n\n\n\n\n\n\n\n\n\n\nDerivation of the Klein Gordon Equation\n\n\n\nquantum mechanics\n\n\n\n\n\n\n\n\n\nDec 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nThe Distribution Shift Problem in LLMs\n\n\n\nartificial intelligence\n\n\n\n\n\n\n\n\n\nNov 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nThe completeness relation in quantum mechanics\n\n\n\nquantum mechanics\n\n\n\n\n\n\n\n\n\nNov 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nThe Differences Between the Computer and the Brain\n\n\n\nartificial intelligence\n\n\n\nSome notes on where von Neumann was right and wrong.\n\n\n\n\n\nNov 21, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nThe Expectation Value of An Observable in Quantum mechanics\n\n\n\nquantum mechanics\n\n\n\n\n\n\n\n\n\nNov 21, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post2/index.html",
    "href": "posts/post2/index.html",
    "title": "The Differences Between the Computer and the Brain",
    "section": "",
    "text": "I’ve recently just finished reading the book “The Computer and The Brain” by John Von Neumann. A brilliant physicist, he contributed a great many things to quantum mechanics, mathematics, economics, and computer science. The book that he wrote was his opinions and observations on the differences between hardware of the digital computers that had at the time, only recently began being developed and the biological “wetware” that is the brain. Interestingly, this book was a collection of his notes that he was working on just before he died. It may very well be the case that he could have had alot more to say.\nHere I’m going to talk about some major points that I thought were interesting in the book, as well as a comparison between how the technology looked back then and the developments that have been made since.\n\nError Resilience as a Trade off of Precision\n\nIn the book, von Neumann highlights an interesting thought: that error resilience is a trade off of precision. To elaborate on this, one could consider a digital machine. In any such machine, the execution of said machine depends on the information stored in memory and the specific sequence of instructions that the computer executes. It’s imperative that these digits are to be represented exactly as the programmer intends. In the case that one is flipped (say from a cosmic particle or something) could change the value of a store number entirely or possibly even crash a program. The brain, on the other hand, is not as vulnerable to tiny perturbations. A spike recieved at time \\(t\\) will not differ in a very meaningful way if that spike were offset to time \\(t+\\Delta t\\) for some small \\(\\Delta t\\). It should be noted that this noise tolerance comes at a tradeoff of precision.\nTo consider what exactly we mean by precision, we can consider the quantity of jitter. Suppose we have a spike train denoted by the times \\(\\{ t_0, t_1, \\dots, t_k\\}\\). We can take the interspike intervals and group them into another list \\(\\{i_0, i_1, \\dots, i_{k-1}\\}\\). Taking the standard deviation of this list we have jitter. We can use this as a measure of timing precision in a neuron. In contrast, a clock edge in a digital computer must have a jitter of \\(10^{-12}\\) seconds, otherwise the effects cascade downards and calculations get messed up. Comparing this with the precision of a neuron, is a difference in 9 orders of magnitude. Clearly, the neuron’s timing fidelity is much less sensitive than a digital computer is required to be.\n\nSerial versus Parallel Organization and Architecture Differences\n\nDigital programs (typically) execute programs sequentially, reflecting its serial nature. Brains on the other hand operate in millisecond level precisions as we have established. Despite this seemingly superior advantage of digital computers, parallelism is what makes the brain able to create rich, semantic representations. Control is decentralized, and computation arises from large networks of activity rather than symbolic manipulation.\nThe von Neumann machine has separated memory and computation units, whereas the brain has them encoded into the same fundamental unit. Synapses store long term data, neurons both compute and store nonlinearities.\n\nDifferent Notational Systems and Short Codes\n\nNeumann estimated t\n\\[\n\\]"
  },
  {
    "objectID": "posts/post7/index.html",
    "href": "posts/post7/index.html",
    "title": "Non Degenerate First Order Perturbation Theory",
    "section": "",
    "text": "Perturbation theory is the study of solving problems that are almost solvable, where the system that you’re trying to solve for is close to one that can be solved exactly.\nFirst, we start with nondegenerate perturbation theory, where\n\\[\nH_{0} \\ket{n^{(0)}} = E^{(0)}_n \\ket{n^{(0)}}\n\\]\nwhere the nondegeneracy condition requires that\n\\[\nE^{(0)}_n \\neq E^{(0)}_m\n\\]\nfor all \\(n \\neq m\\). We start from the time independent Schrodinger equation in operator form\n\\[\nH\\ket{n}=E_n \\ket{n}\n\\]\nwhere our Hamiltonian is expanded in terms of the perturbation parameterized by \\(\\lambda\\)\n\\[\nH=H_0+\\lambda V\n\\]\nIt’s important to note here that V is not the potential, but the pertubation that we are adding to see how our energies and solutions are affected. \\\nThe idea then is to expand the energies and solution kets in a power series\n\\[\nE_n=E^{(0)}_n+\\lambda E^{(1)}_n+\\lambda^2 E^{(2)}_n + \\dots\n\\]\n\\[\n\\ket{n}=\\ket{n^{(0)}}+\\lambda \\ket{n^{(1)}}+\\lambda^2 \\ket{n^{(2)}} + \\dots\n\\]\nPlugging these relations back into the time independent Schrodinger Equation\n\\[\n(H_0+\\lambda V)(\\ket{n^{(0)}}+\\lambda \\ket{n^{(1)}}+\\lambda^2 \\ket{n^{(2)}} + \\dots)\n=(E^{(0)}_n+\\lambda E^{(1)}_n+\\lambda^2 E^{(2)}_n + \\dots)(\\ket{n^{(0)}}+\\lambda \\ket{n^{(1)}}+\\lambda^2 \\ket{n^{(2)}} + \\dots)\n\\]\nIf you expand this out, well the terms would be infinite. However, we can group this expansion in terms of their order in \\(\\lambda\\).\nFirst order non degenerate perturbation theory\nFor first order perturbation theory, we only consider the terms above that are linear in \\(\\lambda\\). Expanding it out, we have that in order 0 in \\(\\lambda\\), we have\n\\[\nH_0\\ket{n^{(0)}}=E^{(0)}_n\\ket{n^{(0)}}\n\\]\nwhich we have taken to already be true. For order 1 in \\(\\lambda\\), we have\n\\[\nV\\ket{n^{(0)}}+H_0\\ket{n^{(1)}}=E^{(0)}_n\\ket{n^{(1)}}+E^{(1)}_n\\ket{n^{(0)}}\n\\]\n\\[\n(V-E^{(1)}_n)\\ket{n^{(0)}}=(E^{(0)}_n-H_0)\\ket{n^{(1)}}\n\\]\nTaking the inner product, we have\n\\[\n\\bra{n^{(0)}}(V-E^{(1)}_n)\\ket{n^{(0)}}=\\bra{n^{(0)}}(E^{(0)}_n-H_0)\\ket{n^{(1)}}\n\\]\n\\[\n\\bra{n^{(0)}}V\\ket{n^{(0)}}-E^{(1)}_n=0\n\\]\n\\[\n\\boxed{E^{(1)}_n=\\bra{n^{(0)}}V\\ket{n^{(0)}}}\n\\]\nThe first order energy correction. To find the expression for the first order solution correction \\(\\ket{n^{(1)}}\\) we start from the order 1 expression again\n\\[\nV\\ket{n^{(0)}}+H_0\\ket{n^{(1)}}=E^{(0)}_n\\ket{n^{(1)}}+E^{(1)}_n\\ket{n^{(0)}}\n\\]\nMultiply both sides by \\(\\bra{m^{0}}\\) where \\(m \\neq n\\)\n\\[\n\\bra{m^{0}}V\\ket{n^{(0)}}+\\bra{m^{0}}H_0\\ket{n^{(1)}}=\\bra{m^{0}}E^{(0)}_n\\ket{n^{(1)}}+\\bra{m^{0}}E^{(1)}_n\\ket{n^{(0)}}\n\\]\n\\[\n\\bra{m^{0}}V\\ket{n^{(0)}}+E^{(0)}_m\\braket{m^{0}|n^{(1)}}=E^{(0)}_n\\braket{m^{0}|n^{(1)}}\n\\]\n\\[\n\\bra{m^{0}}V\\ket{n^{(0)}}=(E^{(0)}_n-E^{(0)}_m)\\braket{m^{0}|n^{(1)}}\n\\]\n\\[\n\\braket{m^{0}|n^{(1)}}=\\frac{\\bra{m^{0}}V\\ket{n^{(0)}}}{(E^{(0)}_n-E^{(0)}_m)}\n\\]\nWith some arguments, if we impose that \\(\\braket{n^{(0)}|n}=1\\), then \\(\\braket{n^{(0)}|n^{(1)}}=0\\) and we have\n\\[\n\\boxed{\\ket{n^{(1)}} = \\sum_{m \\neq n} \\frac{\\bra{m^{(0)}} V \\ket{n^{(0)}}}{E_n^{(0)} - E_m^{(0)}} \\ket{m^{(0)}}}\n\\]"
  },
  {
    "objectID": "posts/post4/index.html",
    "href": "posts/post4/index.html",
    "title": "The completeness relation in quantum mechanics",
    "section": "",
    "text": "A very simple but useful relation in quantum mechanics is the so called completeness relation. Here, I will derive it\nFor any \\(\\ket{\\psi}\\) living in a Hilbert space, you can represent the vector as a linear combination of some orthonormal bases\n\\[\n\\ket{\\psi} = \\sum_{i} c_{\\alpha_i} \\ket{\\alpha_i}\n\\]\nFor any one of these orthonormal bases, applying the bra \\(\\bra{\\alpha_k}\\) with the orthonormality condition \\(\\braket{\\alpha_i | \\alpha_j} = \\delta_{ij}\\)\n\\[\n\\braket{\\alpha_k | \\psi} = \\bra{\\alpha_k} (\\sum_{i} c_{\\alpha_i} \\ket{\\alpha_i})\n\\]\n\\[\n\\braket{\\alpha_k | \\psi} = c_{\\alpha_k}\n\\]\nSo we can rewrite this\n\\[\n\\ket{\\psi} = \\sum_{i} c_{\\alpha_i} \\ket{\\alpha_i}\n\\]\nas\n\\[\n\\ket{\\psi} = \\sum_{i} \\ket{\\alpha_i} \\braket{\\alpha_i | \\psi}\n\\]\nSince \\(\\ket{\\psi}\\) is arbitrary, we have\n\\[\n\\mathbb{I} = \\sum_{i} \\ket{\\alpha_i} \\bra{\\alpha_i}\n\\]\nThe completeness relation. For the continuous case, you have the integral\n\\[\n\\mathbb{I} = \\int da \\ket{a} \\bra{a}\n\\]"
  },
  {
    "objectID": "posts/post8/index.html",
    "href": "posts/post8/index.html",
    "title": "Crossroads",
    "section": "",
    "text": "Over the past few days, I’ve been talking to some friends about this sort of mini crisis that I’m having. In being interested in active matter, I thought that understanding theoretical condensed matter would probably be a good thing to do. You don’t need every aspect of theoretical condensed matter physics to do work in active matter - it’s probably overkill. In fact, I think the expertise of a chemist is probably more suited for work in active matter than a theoretical physicist. But this has encouraged my pursuit in learning theoretical physics anyways - for the reasons that 1. I’m interested in theoretical physics and 2. I know that it will be useful in my pursuit of AI down the line. However, there is another problem that has caught my attention, which is quantum gravity. Quantum gravity is interesting as a problem, first because it’s literally a theory of everything. If one were able to produce such a theory that unifies the Standard Model as well as gravity, you’d essentially have a theory of everything - one model that when given the parameters would tell you how the universe evolves. Being ambitious, this obviously sounds like something interesting to work on. So part of my recent struggles has been attempting to reconcile these two interests: my pursuit of artificial intelligence and this problem of quantum gravity. After much toil and thinking, I’m thinking that it probably can’t be done. At least, with my current viewpoints on how AI progress should be made as well as the nature of QG. They’re just too different. Let me elaborate on how I view both of these.\nMy current perspective on artificial intelligence is that everyone is too LLM-pilled. Alot of time and effort has gone to studying these systems which granted, given their very impressive results since they were first released in 2017 as well as their subsequent scaling by OpenAI and various other companies has led to systems that can perform well on quite alot of tasks. To name some, I’ll include essay writing, image generation/art (not exactly LLM based but can be combined with LLM querying), generally complex reasoning (solving complex maths, physics, and other science problems). I think I should even improve on my own understanding of LLMs - even though I can code them there’s still alot of theory to be learned. Although with these tremendous advantages artificial systems still have a whole slew of tasks that they suck at compared to humans. These include sensorimotor tasks (how would one even solve sensorimotor tasks with LLMS?), commonsense reasoning, and general creativity and pattern recognition. These observations can be summarized by Moravec’s Paradox. It almost seems like there’s a sort of “mutual orthogonality” of computational tasks that are solved by two parallel systems - one that’s the digital computer, and the other the human brain.\nMoravec’s Paradox has been a subject of both fascination and an integral part of my own research recently. The hypothesis is that the cause for it is due to the two different hardware systems that computational tasks are performed on. In other words, hardware is the issue. The paradox has been observed in every computational task that has been optimized on digital computers, and this fact has not changed since their inception in the 50s. Without going into too much detail, I forsee a revolution where the hardware is completely overturned, both in terms of the computational model that they implement as well as the materials that constitute them, possibly expanding to areas where the dominant computers are not made with silicon but something more exotic or unconventional by today’s standards (photonics is an interesting direction). Hence my interest in condensed matter - being well educated in how materials work is part of my desire to bring about this revolution.\nMy motivation in high energy theoretical physics is completely independent from AI. As interesting as the problem of QG is, I don’t think it can be reconciled with pursing artificial intelligence. Understanding quantum gravity first of all, requires an understanding of exotic mathematical structures, an understanding of the Standard Model and quantum field theory which would take me more years to develop, as well as an understanding of the current approaches such as string theory and lqg and their possible flaws. While I could very well forsee myself learning all of these things, it would certainly take out of my AI ambitions. They’re just along two different axes - one is in the land of theory and mathematics, the other in the axis of engineering and computer science. It is a sad but unfortunate reality - that I’d be spreading myself too thin if I tried to do both. For now, the two paths are along the same axes - quantum field theory (as established by my discussion before), but at some point I will have to step back to the AI path. Maybe during retirement I can return to this - if it hasn’t been solved already. Maybe."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Timothy Oh",
    "section": "",
    "text": "Hi, I am Timothy Oh. Currently I am doing research at the University of California, Riverside. I’m fascinated by how intelligence can emerge as a consequence of simpler parts. This reflects my belief that our understanding of intelligence is best obtained through a bottom-up approach, where we should try our best to build one, in contrast to more systematic approaches like neuronal-population averaging and statistical abstractions.\nMy expertise includes machine learning, theoretical and computational neuroscience, neuromorphic computing, computational complexity theory, software engineering, computer architecture, electrical engineering, computational biology, information theory, stem cell research, and alternative forms of computation. I also have an interest in physics, with a very rudimentary understanding of general relativity and quantum field theory, and their applications to exciting frontiers such as nuclear fusion, quantum computing, gravitational wave detection, and high energy particle accelerators.\nI plan on using this website as a home; to write down half baked thoughts, talk about science and research directions, personal life, philosophy, and concrete notes on whatever I’m learning at the moment.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n “It also ought to be noted that the language here involved may well correspond to a short code in the sense described earlier, rather than to a complete code: when we talk mathematics, we may be discussing a secondary language, built on the primary language truly used by the central nervous system.” \n\n- John Von Neumann, The Computer and the Brain, 1958\n\n\n\nDownload CV\n\n\\[\n\\]"
  }
]